{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnlpconda58a66029c51549b7b2d39520295f4ea7",
   "display_name": "Python 3.8.5 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preparaci√≥n del Dataset\n",
    "---\n",
    "\n",
    "- Se obtienen y preprocesan textos directos del dataset para luego ser persistidos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from document import Document\n",
    "import util.log as log\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Manager\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "log.init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 documents were processed. 0 documents errored.\n"
     ]
    }
   ],
   "source": [
    "corpora = []\n",
    "errors = []\n",
    "\n",
    "def process_file(file, corpus):\n",
    "    try:\n",
    "        path = '{}{}'.format(directory, file)\n",
    "        document = Document(path)\n",
    "        splitted_file = file.split(' $ ') \n",
    "        topic = splitted_file[0]\n",
    "        title = splitted_file[1]\n",
    "        corpus.append([path, title, document.string, document.word_count(), document.type_count(), document.sentences,\n",
    "                document.lemmatized_string, document.stemmed_string, document.simple_preprocessed_string, topic, document.named_entities])\n",
    "    except InvalidDocument:\n",
    "        errors.append(file)\n",
    "\n",
    "directory = 'labeled_dataset/'\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "ps = []\n",
    "\n",
    "dataframe = None\n",
    "shared_corpora = None\n",
    "with Manager() as manager:\n",
    "\n",
    "    shared_corpora = manager.list()\n",
    "\n",
    "    for file in files[:2]:\n",
    "        p = Process(target = process_file, args=(file,shared_corpora))\n",
    "        p.start()\n",
    "        ps.append(p)\n",
    "\n",
    "    for p in ps:\n",
    "        p.join()\n",
    "\n",
    "    corpora = list(shared_corpora)\n",
    "\n",
    "\n",
    "print('{} documents were processed. {} documents errored.'.format(len(corpora), len(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data = corpora, columns=['original_path', 'document_title', 'string', 'word_count', 'type_count', 'sentences','lemmatized_text', 'stemmed_text', 'simple_preprocessed', 'topic', 'named_entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}