{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnlpconda58a66029c51549b7b2d39520295f4ea7",
   "display_name": "Python 3.8.5 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preparaci√≥n del Dataset\n",
    "---\n",
    "\n",
    "- Se obtienen y preprocesan textos directos del dataset para luego ser persistidos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from document import Document\n",
    "import util.log as log\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Manager\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "log.init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = []\n",
    "init_time = datetime.now()\n",
    "directory = 'labeled_dataset/'\n",
    "files = os.listdir(directory)\n",
    "dataframe = None\n",
    "number_of_cores_to_use = 10\n",
    "\n",
    "def process_file(file):\n",
    "    log.info('Processing file: {}'.format(file))\n",
    "    try:\n",
    "        path = '{}{}'.format(directory, file)\n",
    "        document = Document(path)\n",
    "        splitted_file = file.split(' $ ') \n",
    "        topic = splitted_file[0]\n",
    "        title = splitted_file[1]\n",
    "        return [path, title, document.string, document.word_count(), document.type_count(), document.sentences,\\\n",
    "                       document.lemmatized_string, document.stemmed_string, document.simple_preprocessed_string, topic, \\\n",
    "                       document.named_entities, document.bigrams, document.trigrams, document.lemmatized_bigrams, \\\n",
    "                       document.lemmatized_trigrams, document.stemmed_bigrams, document.stemmed_trigrams, \\\n",
    "                       document.simple_preprocessed_bigrams, document.simple_preprocessed_trigrams]\n",
    "    except InvalidDocument:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "with Pool(number_of_cores_to_use) as pool:\n",
    "    corpora = pool.map(process_file, files)\n",
    "\n",
    "time = datetime.now() - init_time\n",
    "log.info('{} documents were processed. {} documents errored. Total time used: {}, Total cores used: {}'.format(len(corpora), len(errors), \\\n",
    "                                                                                                          str(time), number_of_cores_to_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data = corpora, columns=['original_path', 'document_title', 'string', 'word_count', 'type_count', 'sentences','lemmatized_text', 'stemmed_text', 'simple_preprocessed', 'topic', 'named_entities', 'tokens_bigrams', 'tokens_trigrams', 'lemmatized_bigrams', 'lemmatized_trigrams', 'stemmed_bigrams', 'stemmed_trigrams', 'simple_preprocessed_bigrams', 'simple_preprocessed_trigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}