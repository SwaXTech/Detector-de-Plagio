{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnlpconda58a66029c51549b7b2d39520295f4ea7",
   "display_name": "Python 3.8.5 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Naive Bayes\n",
    "\n",
    "## Modelo de clasificación de textos de manera supervisada\n",
    "\n",
    "- [Documentación utilizada](https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b#:~:text=The%20Naive%20Bayes%20classifier%20is,time%20and%20less%20training%20data)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "## Preparación del dataset\n",
    "\n",
    "- Obtención del dataset de un `CSV`\n",
    "- Ignoramos las columnas innecesarias\n",
    "- Limpieza de datos\n",
    "- Separación de dataset en Train/Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Teorema de Bayes\n",
    "\n",
    "![](https://miro.medium.com/max/358/1*8vBP06EtIIf-420o_q1u6g.png)\n",
    "\n",
    "\n",
    "Se debe calcular qué tópico tiene mayor probabilidad para una texto determinado\n",
    "\n",
    "¿`P(c1 | unTexto)` es mayor que `P(c2 | unTexto)`?\n",
    "\n",
    "Según el Teorema de Bayes, esto se puede calcular de la siguiente manera:\n",
    "\n",
    "`P(c | unTexto) = (P(unTexto | c) * P(c)) / P(unTexto)`\n",
    "\n",
    "Como para ambas clases el denominador es el mismo, podemos ignorarlo y nos queda:\n",
    "\n",
    "`P(c | unTexto) = P(unTexto | c) * P(c)`\n",
    "\n",
    "Finalmente\n",
    "\n",
    "`P(c) = count(textos, c) / count(textos, dataset)` \n",
    "\n",
    "`P(unTexto) = count(unTexto, c) / count(textos, c)`\n",
    "\n",
    "Dado que los textos a evaluar no necesariamente aparecen en el dataset, y por consiguiente su probabilidad es cero. Entonces se asumen todas las palabras independientes. Ésto se lo conoce como [Markov Assumption](https://es.wikipedia.org/wiki/Proceso_de_M%C3%A1rkov)\n",
    "\n",
    "Entonces teniendo en cuenta lo mencionado:\n",
    "\n",
    "`P(unTexto | c) = P(w1 | c) * P(w2 | c) * ... * P(wn | c)`\n",
    "\n",
    "Siendo\n",
    "\n",
    "`P(unaPalabra | c) = count(unaPalabra, c) / count(palabras, c)`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from repository.csv_tools import get_documents\n",
    "import random\n",
    "from util.count_vectorizer import MyCountVectorizer\n",
    "import math\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "source": [
    "## Preparación del dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documents('../data.csv')\n",
    "preprocessed_docs = [(document.lemmatized_string, document.topic) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(docs, test_size):\n",
    "    random.shuffle(docs)\n",
    "\n",
    "    test = int(len(docs) * test_size)\n",
    "    texts_train = [doc[0] for doc in docs[test:]]\n",
    "    topic_train = [doc[1] for doc in docs[test:]]\n",
    "    texts_test  = [doc[0] for doc in docs[:test]]\n",
    "    topic_test  = [doc[1] for doc in docs[:test]]\n",
    "\n",
    "    return texts_train, topic_train, texts_test, topic_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, classes_train, text_test, classes_test = split_dataset(preprocessed_docs, 0.2)"
   ]
  },
  {
   "source": [
    "## Entrenamiento"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-051936885056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtouple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0m__frequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__features_by_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtouple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__frequencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "classes = set(classes_train)\n",
    "\n",
    "def get_documents_from_class(texts, classes, topic):\n",
    "    return [texts[index] for index, cl in enumerate(classes_train) if cl == topic]\n",
    "\n",
    "def get_words(vectorizer):\n",
    "    return vectorizer.get_feature_names(); \n",
    "\n",
    "def get_words_count(term_document_matrix):\n",
    "    return term_document_matrix.toarray().sum(axis=0) \n",
    "\n",
    "def get_frecuencies(word_list, count_list):\n",
    "    return dict(zip(word_list, count_list))\n",
    "\n",
    "def get_probabilities(word_list, count_list):\n",
    "    prob = [(count / len(word_list)) for count in count_list]\n",
    "    return dict(zip(word_list, prob))\n",
    "\n",
    "def get_features_count(count_list):\n",
    "    return count_list.sum(axis=0)\n",
    "\n",
    "def process(topic):\n",
    "    documents = get_documents_from_class(texts_train, classes_train, topic)\n",
    "\n",
    "    vectorizer           = MyCountVectorizer(preprocess = False)\n",
    "    term_document_matrix = vectorizer.fit_transform(documents)\n",
    "    word_list            = get_words(vectorizer)\n",
    "    count_list           = get_words_count(term_document_matrix)\n",
    "\n",
    "    __frequencies = get_frecuencies(word_list, count_list)\n",
    "    __probabilities = get_probabilities(word_list, count_list)\n",
    "    __features_by_class = get_features_count(count_list)\n",
    "   \n",
    "    return (__frequencies, __probabilities, __features_by_class, topic)\n",
    "    \n",
    "\n",
    "with Pool(10) as pool:\n",
    "    results = pool.map(process, classes)\n",
    "\n",
    "for result in results:\n",
    "    topic = result[3]\n",
    "    frequencies[topic] = result[0]\n",
    "    probabilities[topic] = result[1]\n",
    "    features_by_class[topic] = result[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features_in_training_set = {}\n",
    "vectorizer = MyCountVectorizer(preprocess = False)\n",
    "tdm = vectorizer.fit_transform(texts_train)\n",
    "total_features = len(vectorizer.get_feature_names())\n",
    "print('Total features in training set: ' + total_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probabilities = {}\n",
    "\n",
    "for topic in set(classes_train):\n",
    "    class_probabilities[topic] = classes_train.count(topic) / len(classes_train)\n"
   ]
  },
  {
   "source": [
    "## Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_word_probability(word, topic, frequencies):\n",
    "    freq = frequencies[topic]\n",
    "    count = freq[word] if word in freq.keys() else 0\n",
    "    return (count + 1) / (features_by_class[topic] + total_features)\n",
    "\n",
    "def get_words_probabilities(document, topic, frequencies):\n",
    "    prob = []\n",
    "    for word in document:\n",
    "        probability = get_word_probability(word, topic, frequencies)\n",
    "        prob.append(probability)\n",
    "    return dict(zip(document, prob))\n",
    "\n",
    "def get_topic_predicted(predicted_probabilities):\n",
    "\n",
    "    max_probability = None\n",
    "    topic = None\n",
    "\n",
    "    for key in predicted_probabilities.keys():\n",
    "        p = predicted_probabilities[key] \n",
    "        if not max_probability or p > max_probability:\n",
    "            max_probability = p\n",
    "            topic = key\n",
    "    \n",
    "    return topic, max_probability\n",
    "\n",
    "\n",
    "for ix, document in enumerate(text_test[:1]):\n",
    "\n",
    "    predicted_probabilities = {}\n",
    "\n",
    "    for topic in classes:\n",
    "        words_probability = get_words_probabilities(document, topic, frequencies)\n",
    "        P = math.log(class_probabilities[topic], 10)\n",
    "        for key in words_probability.keys():\n",
    "            p = words_probability[key]\n",
    "            P += math.log(p, 10)\n",
    "        predicted_probabilities[topic] = P\n",
    "    \n",
    "    topic_predicted, probability_predicted = get_topic_predicted(predicted_probabilities)\n",
    "    print(predicted_probabilities)\n",
    "    print('For document #{}: Predicted topic: {} with P = {}. Real topic: {}'.format(ix, topic_predicted, probability_predicted, classes_train[ix]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}