{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnlpconda58a66029c51549b7b2d39520295f4ea7",
   "display_name": "Python 3.8.5 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Naive Bayes\n",
    "\n",
    "## Modelo de clasificación de textos de manera supervisada\n",
    "\n",
    "- [Documentación utilizada](https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b#:~:text=The%20Naive%20Bayes%20classifier%20is,time%20and%20less%20training%20data)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "## Preparación del dataset\n",
    "\n",
    "- Obtención del dataset de un `CSV`\n",
    "- Ignoramos las columnas innecesarias\n",
    "- Limpieza de datos\n",
    "- Separación de dataset en Train/Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Teorema de Bayes\n",
    "\n",
    "![](https://miro.medium.com/max/358/1*8vBP06EtIIf-420o_q1u6g.png)\n",
    "\n",
    "\n",
    "Se debe calcular qué tópico tiene mayor probabilidad para una texto determinado\n",
    "\n",
    "¿`P(c1 | unTexto)` es mayor que `P(c2 | unTexto)`?\n",
    "\n",
    "Según el Teorema de Bayes, esto se puede calcular de la siguiente manera:\n",
    "\n",
    "`P(c | unTexto) = (P(unTexto | c) * P(c)) / P(unTexto)`\n",
    "\n",
    "Como para ambas clases el denominador es el mismo, podemos ignorarlo y nos queda:\n",
    "\n",
    "`P(c | unTexto) = P(unTexto | c) * P(c)`\n",
    "\n",
    "Finalmente\n",
    "\n",
    "`P(c) = count(textos, c) / count(textos, dataset)` \n",
    "\n",
    "`P(unTexto) = count(unTexto, c) / count(textos, c)`\n",
    "\n",
    "Dado que los textos a evaluar no necesariamente aparecen en el dataset, y por consiguiente su probabilidad es cero. Entonces se asumen todas las palabras independientes. Ésto se lo conoce como [Markov Assumption](https://es.wikipedia.org/wiki/Proceso_de_M%C3%A1rkov)\n",
    "\n",
    "Entonces teniendo en cuenta lo mencionado:\n",
    "\n",
    "`P(unTexto | c) = P(w1 | c) * P(w2 | c) * ... * P(wn | c)`\n",
    "\n",
    "Siendo\n",
    "\n",
    "`P(unaPalabra | c) = count(unaPalabra, c) / count(palabras, c)`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from repository.csv_tools import get_documents\n",
    "import random\n",
    "from util.count_vectorizer import MyCountVectorizer\n",
    "import math\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "source": [
    "## Preparación del dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documents('../data.csv')\n",
    "preprocessed_docs = [(document.lemmatized_string, document.topic) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(docs, test_size):\n",
    "    random.shuffle(docs)\n",
    "\n",
    "    test = int(len(docs) * test_size)\n",
    "    texts_train = [doc[0] for doc in docs[test:]]\n",
    "    topic_train = [doc[1] for doc in docs[test:]]\n",
    "    texts_test  = [doc[0] for doc in docs[:test]]\n",
    "    topic_test  = [doc[1] for doc in docs[:test]]\n",
    "\n",
    "    return texts_train, topic_train, texts_test, topic_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, classes_train, text_test, classes_test = split_dataset(preprocessed_docs, 0.2)"
   ]
  },
  {
   "source": [
    "## Entrenamiento"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = set(classes_train)\n",
    "\n",
    "def get_documents_from_class(texts, classes, topic):\n",
    "    return [texts[index] for index, cl in enumerate(classes_train) if cl == topic]\n",
    "\n",
    "def get_words(vectorizer):\n",
    "    return vectorizer.get_feature_names(); \n",
    "\n",
    "def get_words_count(term_document_matrix):\n",
    "    return term_document_matrix.toarray().sum(axis=0) \n",
    "\n",
    "def get_frecuencies(word_list, count_list):\n",
    "    return dict(zip(word_list, count_list))\n",
    "\n",
    "def get_probabilities(word_list, count_list):\n",
    "    prob = [(count / len(word_list)) for count in count_list]\n",
    "    return dict(zip(word_list, prob))\n",
    "\n",
    "def get_features_count(count_list):\n",
    "    return count_list.sum(axis=0)\n",
    "\n",
    "def process(topic):\n",
    "    documents = get_documents_from_class(texts_train, classes_train, topic)\n",
    "\n",
    "    vectorizer           = MyCountVectorizer(preprocess = False)\n",
    "    term_document_matrix = vectorizer.fit_transform(documents)\n",
    "    word_list            = get_words(vectorizer)\n",
    "    count_list           = get_words_count(term_document_matrix)\n",
    "\n",
    "    __frequencies = get_frecuencies(word_list, count_list)\n",
    "    __probabilities = get_probabilities(word_list, count_list)\n",
    "    __features_by_class = get_features_count(count_list)\n",
    "   \n",
    "    return (__frequencies, __probabilities, __features_by_class, topic)\n",
    "    \n",
    "\n",
    "with Pool(10) as pool:\n",
    "    results = pool.map(process, classes)\n",
    "\n",
    "for result in results:\n",
    "    topic = result[3]\n",
    "    frequencies[topic] = result[0]\n",
    "    probabilities[topic] = result[1]\n",
    "    features_by_class[topic] = result[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total features: 1115\n"
     ]
    }
   ],
   "source": [
    "features = [key for key in frequencies[topic].keys() for topic in classes]\n",
    "\n",
    "total_features = len(set(features))\n",
    "print('Total features: {}'.format(total_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probabilities = {}\n",
    "\n",
    "for topic in set(classes_train):\n",
    "    class_probabilities[topic] = classes_train.count(topic) / len(classes_train)\n"
   ]
  },
  {
   "source": [
    "## Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For document #0: Predicted topic: Sistemas emergentes with P = -1000.8557035744055. Real topic: Sistemas emergentes\n",
      "For document #1: Predicted topic: Wikinomics with P = -2483.940418548769. Real topic: Wikinomics\n",
      "For document #2: Predicted topic: Test with P = -2547.886458839754. Real topic: La larga cola\n",
      "For document #3: Predicted topic: La larga cola with P = -2220.704247597434. Real topic: La larga cola\n",
      "For document #4: Predicted topic: La larga cola with P = -2704.5176637854624. Real topic: La larga cola\n",
      "For document #5: Predicted topic: Adopcion y difusion with P = -1635.249766293545. Real topic: Adopcion y difusion\n",
      "For document #6: Predicted topic: Sistemas emergentes with P = -896.2390487986557. Real topic: Sistemas emergentes\n",
      "For document #7: Predicted topic: Python with P = -4987.274582167257. Real topic: Marketing 4.0\n",
      "For document #8: Predicted topic: Sistemas emergentes with P = -1135.4272116586253. Real topic: Sistemas emergentes\n",
      "For document #9: Predicted topic: La larga cola with P = -1798.3883575885945. Real topic: La larga cola\n",
      "For document #10: Predicted topic: La larga cola with P = -1725.6007453198324. Real topic: La larga cola\n",
      "For document #11: Predicted topic: E-commerce with P = -1631.7882301262648. Real topic: E-commerce\n",
      "For document #12: Predicted topic: Economia de experiencia with P = -2288.789955763081. Real topic: Economia de experiencia\n",
      "For document #13: Predicted topic: La sociedad de costo marginal cero with P = -2544.356611567264. Real topic: La sociedad de costo marginal cero\n",
      "For document #14: Predicted topic: Adopcion y difusion with P = -2481.9168849684656. Real topic: Adopcion y difusion\n",
      "For document #15: Predicted topic: La larga cola with P = -2175.225421875858. Real topic: La larga cola\n",
      "For document #16: Predicted topic: E-commerce with P = -1660.8495960263424. Real topic: E-commerce\n",
      "For document #17: Predicted topic: Marketing 4.0 with P = -3474.2290330327537. Real topic: Marketing 4.0\n",
      "For document #18: Predicted topic: La larga cola with P = -2058.9996291138823. Real topic: La larga cola\n",
      "For document #19: Predicted topic: Nueva economia with P = -1263.4586240736312. Real topic: Nueva economia\n",
      "For document #20: Predicted topic: Economia de experiencia with P = -1324.1517154556323. Real topic: Economia de experiencia\n",
      "For document #21: Predicted topic: La larga cola with P = -1233.8668317613574. Real topic: La larga cola\n",
      "For document #22: Predicted topic: Sistemas emergentes with P = -1024.4331409529032. Real topic: Sistemas emergentes\n",
      "For document #23: Predicted topic: E-commerce with P = -2323.5894560581496. Real topic: E-commerce\n",
      "For document #24: Predicted topic: Nueva economia with P = -1000.3377899343118. Real topic: Nueva economia\n",
      "For document #25: Predicted topic: La sociedad de costo marginal cero with P = -1921.4392291574882. Real topic: La sociedad de costo marginal cero\n",
      "For document #26: Predicted topic: Wikinomics with P = -2748.9662741172097. Real topic: Wikinomics\n",
      "For document #27: Predicted topic: Adopcion y difusion with P = -2989.0100402013527. Real topic: Adopcion y difusion\n",
      "For document #28: Predicted topic: Wikinomics with P = -1721.1585107127364. Real topic: Wikinomics\n",
      "For document #29: Predicted topic: Adopcion y difusion with P = -2797.0482495159713. Real topic: Adopcion y difusion\n",
      "For document #30: Predicted topic: La larga cola with P = -2417.336943959526. Real topic: La larga cola\n",
      "For document #31: Predicted topic: Nueva economia with P = -597.3091618453257. Real topic: Nueva economia\n",
      "For document #32: Predicted topic: Economia de experiencia with P = -2883.5821689003396. Real topic: Economia de experiencia\n",
      "For document #33: Predicted topic: Adopcion y difusion with P = -2208.117680329331. Real topic: Adopcion y difusion\n",
      "For document #34: Predicted topic: La sociedad de costo marginal cero with P = -1829.4654203999512. Real topic: La sociedad de costo marginal cero\n",
      "For document #35: Predicted topic: Economia de experiencia with P = -1132.9717655510567. Real topic: Economia de experiencia\n",
      "For document #36: Predicted topic: La larga cola with P = -2192.510688929334. Real topic: La larga cola\n",
      "For document #37: Predicted topic: Sistemas emergentes with P = -566.2927039476223. Real topic: Sistemas emergentes\n",
      "For document #38: Predicted topic: Nueva economia with P = -1301.4180262839025. Real topic: Nueva economia\n",
      "For document #39: Predicted topic: Economia de experiencia with P = -1942.1216037575348. Real topic: Economia de experiencia\n",
      "For document #40: Predicted topic: Adopcion y difusion with P = -2208.117680329331. Real topic: Adopcion y difusion\n",
      "For document #41: Predicted topic: Machine - Platform - Crowd with P = -1703.5538328877797. Real topic: Machine - Platform - Crowd\n",
      "For document #42: Predicted topic: Economia de experiencia with P = -3154.1442132388524. Real topic: Economia de experiencia\n",
      "For document #43: Predicted topic: La sociedad de costo marginal cero with P = -2331.8714744573313. Real topic: La sociedad de costo marginal cero\n",
      "For document #44: Predicted topic: Adopcion y difusion with P = -1822.9365949517942. Real topic: Adopcion y difusion\n",
      "For document #45: Predicted topic: Marketing 4.0 with P = -2894.990977972828. Real topic: Marketing 4.0\n",
      "For document #46: Predicted topic: Adopcion y difusion with P = -960.4420919719078. Real topic: Adopcion y difusion\n",
      "For document #47: Predicted topic: Economia de experiencia with P = -2934.804517828397. Real topic: Economia de experiencia\n",
      "For document #48: Predicted topic: Wikinomics with P = -2839.8292944987643. Real topic: Wikinomics\n",
      "For document #49: Predicted topic: La larga cola with P = -3221.887165060821. Real topic: La larga cola\n",
      "For document #50: Predicted topic: Economia de experiencia with P = -2306.2220308556443. Real topic: Economia de experiencia\n",
      "For document #51: Predicted topic: Economia de experiencia with P = -3012.2795296758886. Real topic: Economia de experiencia\n",
      "For document #52: Predicted topic: Adopcion y difusion with P = -2054.009988242658. Real topic: Adopcion y difusion\n",
      "For document #53: Predicted topic: Adopcion y difusion with P = -3055.6950694619673. Real topic: Adopcion y difusion\n",
      "For document #54: Predicted topic: Nueva economia with P = -1890.7182207714773. Real topic: Nueva economia\n",
      "For document #55: Predicted topic: Marketing 4.0 with P = -2894.990977972828. Real topic: Marketing 4.0\n",
      "For document #56: Predicted topic: Sistemas emergentes with P = -1381.3588358967381. Real topic: Sistemas emergentes\n",
      "For document #57: Predicted topic: Sistemas emergentes with P = -823.4598118374605. Real topic: Sistemas emergentes\n",
      "For document #58: Predicted topic: Economia de experiencia with P = -1101.7189534192846. Real topic: Economia de experiencia\n",
      "For document #59: Predicted topic: E-commerce with P = -1585.3851253554515. Real topic: E-commerce\n",
      "For document #60: Predicted topic: Economia de experiencia with P = -1977.3160747272077. Real topic: Economia de experiencia\n",
      "For document #61: Predicted topic: Economia de experiencia with P = -2586.1679207541424. Real topic: Economia de experiencia\n"
     ]
    }
   ],
   "source": [
    "def get_word_probability(word, topic, frequencies):\n",
    "    freq = frequencies[topic]\n",
    "    count = freq[word] if word in freq.keys() else 0\n",
    "    return (count + 1) / (features_by_class[topic] + total_features)\n",
    "\n",
    "def get_words_probabilities(document, topic, frequencies):\n",
    "    prob = []\n",
    "    for word in document:\n",
    "        probability = get_word_probability(word, topic, frequencies)\n",
    "        prob.append(probability)\n",
    "    return dict(zip(document, prob))\n",
    "\n",
    "def get_topic_predicted(predicted_probabilities):\n",
    "\n",
    "    max_probability = None\n",
    "    topic = None\n",
    "\n",
    "    for key in predicted_probabilities.keys():\n",
    "        p = predicted_probabilities[key] \n",
    "        if not max_probability or p > max_probability:\n",
    "            max_probability = p\n",
    "            topic = key\n",
    "    \n",
    "    return topic, max_probability\n",
    "\n",
    "\n",
    "for ix, document in enumerate(text_test[:]):\n",
    "\n",
    "    predicted_probabilities = {}\n",
    "\n",
    "    for topic in classes:\n",
    "        words_probability = get_words_probabilities(document, topic, frequencies)\n",
    "        P = math.log(class_probabilities[topic], 2)\n",
    "        for key in words_probability.keys():\n",
    "            p = words_probability[key]\n",
    "            P += math.log(p, 2)\n",
    "        predicted_probabilities[topic] = P\n",
    "    \n",
    "    topic_predicted, probability_predicted = get_topic_predicted(predicted_probabilities)\n",
    "    print('For document #{}: Predicted topic: {} with P = {}. Real topic: {}'.format(ix, topic_predicted, probability_predicted, classes_test[ix]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}