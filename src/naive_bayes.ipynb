{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnlpconda58a66029c51549b7b2d39520295f4ea7",
   "display_name": "Python 3.8.5 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Naive Bayes\n",
    "\n",
    "## Modelo de clasificación de textos de manera supervisada\n",
    "\n",
    "- [Documentación utilizada](https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b#:~:text=The%20Naive%20Bayes%20classifier%20is,time%20and%20less%20training%20data)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "## Preparación del dataset\n",
    "\n",
    "- Obtención del dataset de un `CSV`\n",
    "- Ignoramos las columnas innecesarias\n",
    "- Limpieza de datos\n",
    "- Separación de dataset en Train/Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from repository.csv_tools import get_documents\n",
    "import random\n",
    "from util.count_vectorizer import MyCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documents('../data.csv')\n",
    "preprocessed_docs = [(document.stemmed_string, document.topic) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(docs, test_size):\n",
    "    random.shuffle(docs)\n",
    "\n",
    "    test = int(len(docs) * test_size)\n",
    "    texts_train = [doc[0] for doc in docs[test:]]\n",
    "    topic_train = [doc[1] for doc in docs[test:]]\n",
    "    texts_test  = [doc[0] for doc in docs[:test]]\n",
    "    topic_test  = [doc[1] for doc in docs[:test]]\n",
    "\n",
    "    return texts_train, topic_train, texts_test, topic_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, classes_train, text_test, classes_test = split_dataset(preprocessed_docs, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = set(classes_train)\n",
    "frequencies = {}\n",
    "probabilities = {}\n",
    "features_by_class = {}\n",
    "\n",
    "def get_documents_from_class(texts, classes, topic):\n",
    "    return [texts[index] for index, cl in enumerate(classes_train) if cl == topic]\n",
    "\n",
    "def get_words(vectorizer):\n",
    "    return vectorizer.get_feature_names(); \n",
    "\n",
    "def get_words_count(vectorizer):\n",
    "    return term_document_matrix.toarray().sum(axis=0) \n",
    "\n",
    "def get_frecuencies(word_list, count_list):\n",
    "    return dict(zip(word_list, count_list))\n",
    "\n",
    "def get_probabilities(word_list, count_list):\n",
    "    prob = [(count / len(word_list)) for count in count_list]\n",
    "    return dict(zip(word_list, prob))\n",
    "\n",
    "def get_features_count(count_list):\n",
    "    return count_list.sum(axis=0)\n",
    "\n",
    "for topic in classes:\n",
    "    documents = get_documents_from_class(texts_train, classes_train, topic)\n",
    "\n",
    "    vectorizer           = MyCountVectorizer(preprocess = False)\n",
    "    term_document_matrix = vectorizer.fit_transform(documents)\n",
    "    word_list            = get_words(vectorizer)\n",
    "    count_list           = get_words_count(vectorizer)\n",
    "\n",
    "    frequencies[topic] = get_frecuencies(word_list, count_list)\n",
    "\n",
    "    probabilities[topic] = get_probabilities(word_list, count_list)\n",
    "\n",
    "    features_by_class[topic] = get_features_count(count_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1504"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "total_features_in_training_set = {}\n",
    "vectorizer = MyCountVectorizer(preprocess = False)\n",
    "tdm = vectorizer.fit_transform(texts_train)\n",
    "total_features = len(vectorizer.get_feature_names())\n",
    "total_features\n"
   ]
  },
  {
   "source": [
    "## Teorema de Bayes\n",
    "\n",
    "![](https://miro.medium.com/max/358/1*8vBP06EtIIf-420o_q1u6g.png)\n",
    "\n",
    "\n",
    "Se debe calcular qué tópico tiene mayor probabilidad para una texto determinado\n",
    "\n",
    "¿`P(c1 | unTexto)` es mayor que `P(c2 | unTexto)`?\n",
    "\n",
    "Según el Teorema de Bayes, esto se puede calcular de la siguiente manera:\n",
    "\n",
    "`P(c | unTexto) = (P(unTexto | c) * P(c)) / P(unTexto)`\n",
    "\n",
    "Como para ambas clases el denominador es el mismo, podemos ignorarlo y nos queda:\n",
    "\n",
    "`P(c | unTexto) = P(unTexto | c) * P(c)`\n",
    "\n",
    "Finalmente\n",
    "\n",
    "`P(c) = count(textos, c) / count(textos, dataset)` \n",
    "\n",
    "`P(unTexto) = count(unTexto, c) / count(textos, c)`\n",
    "\n",
    "Dado que los textos a evaluar no necesariamente aparecen en el dataset, y por consiguiente su probabilidad es cero. Entonces se asumen todas las palabras independientes. Ésto se lo conoce como [Markov Assumption](https://es.wikipedia.org/wiki/Proceso_de_M%C3%A1rkov)\n",
    "\n",
    "Entonces teniendo en cuenta lo mencionado:\n",
    "\n",
    "`P(unTexto | c) = P(w1 | c) * P(w2 | c) * ... * P(wn | c)`\n",
    "\n",
    "Siendo\n",
    "\n",
    "`P(unaPalabra | c) = count(unaPalabra, c) / count(palabras, c)`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}